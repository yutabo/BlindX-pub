import sys
import re
import glob
import MeCab
import chardet
from collections import Counter

#MECAB_ARGS = '-d /usr/local/lib/mecab/dic/mecab-ipadic-neologd'
MECAB_ARGS = '-d /opt/homebrew/lib/mecab/dic/mecab-ipadic-neologd'
KEISHO = ['ã•ã‚“', 'å›', 'æ§˜', 'ã¡ã‚ƒã‚“', 'ãã‚“']

def read_text_auto_encoding(filepath):
    with open(filepath, 'rb') as f:
        raw = f.read()
    detected = chardet.detect(raw)
    try:
        return raw.decode(detected['encoding'] or 'utf-8')
    except:
        print(f"âŒ Failed to decode {filepath} with {detected['encoding']}")
        return ""

def extract_named_entities(text):
    tagger = MeCab.Tagger(MECAB_ARGS)
    tagger.parse("")  # MeCab ãƒã‚°å›é¿

    node = tagger.parseToNode(text)
    words = []
    previous_surface = ""
    previous_features = []

    while node:
        surface = node.surface
        features = node.feature.split(',')

        if (
            len(features) > 2 and
            features[0] == "åè©" and
            features[1] == "å›ºæœ‰åè©" and
            features[2] in ["äººå", "åœ°åŸŸ", "çµ„ç¹”"]
        ):
            words.append(surface)

        if surface in KEISHO and previous_surface:
            if (
                len(previous_surface) >= 2 and
                previous_features and
                previous_features[0] == "åè©"
            ):
                words.append(previous_surface)

        previous_surface = surface
        previous_features = features
        node = node.next

    return words

def is_valid_hotword(word, all_words):
    if not word:
        return False
    if re.fullmatch(r'[ã-ã‚“ãƒ¼]+', word):   return False
#    if re.fullmatch(r'[ã-ã‚“]{1,2}', word): return False
    if re.fullmatch(r'[ã‚¡-ãƒ¶ãƒ¼]{1}', word): return False
    if re.fullmatch(r'[a-zA-Zï¼¡-ï¼ºï½-ï½š]{1,3}', word): return False
    # ä¸€æ–‡å­—æ¼¢å­—ã§ä»–ã«å«ã¾ã‚Œã¦ã„ã‚‹
    if re.fullmatch(r'[ä¸€-é¾¯]', word):
        for other in all_words:
            if other != word and word in other:
                return False
    # æ„Ÿå˜†è©ã‚„å¥èª­ç‚¹ãªã©ã®è¨˜å·ï¼ˆï¼ã€â€¦ã€‚ãªã©ï¼‰
    if re.search(r'[ã€‚ã€ï¼ãƒ»â€¦ï¼!ï¼Ÿ?]', word):
        return False
    # ã‚ˆãã‚ã‚‹ä»£åè©çš„ãªèª
    NG_WORDS = {'ãã®å­', 'ãã®äºº', 'ã“ã®å­', 'ã‚ã®äºº', 'ãªï¼', 'ãªã', 'ãµãƒ¼ã‚“', 'ã¸ãˆ', 'ã†ã‚“', 'ã¯ã„', 'ã„ã„ãˆ', 'ã‚ãƒ¼'}
    if word in NG_WORDS:
        return False

    return True

def main():
    all_text = ""
    for pattern in sys.argv[1:]:
        for file in glob.glob(pattern):
            text = read_text_auto_encoding(file)
            if text:
#                print(f"\nğŸ“‚ Reading: {file}")
                all_text += text + "\n"

    raw_words = extract_named_entities(all_text)
#    print(f"\nğŸ“‹ æŠ½å‡ºã•ã‚ŒãŸå›ºæœ‰åè©å€™è£œ: {raw_words}")

    count = Counter(raw_words)
    filtered = [w for w in count if count[w] >= 3 and is_valid_hotword(w, count)]
    filtered = sorted(set(filtered))

    with open("hotwords.txt", "w", encoding="utf-8") as f:
        for word in filtered:
            f.write(word + "\n")

    print(f"\nâœ… {len(filtered)} hotwords extracted to hotwords.txt")




    if filtered:
        print("ğŸ“„ æŠ½å‡ºçµæœ:", filtered)

if __name__ == "__main__":
    main()
