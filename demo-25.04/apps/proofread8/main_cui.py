#@title ã‹ãªæ¼¢å­—å¤‰æ›ãƒ†ã‚¹ãƒˆ
# MIT License (c) 2024, 2025 Masakazu Suzuoki, AxTecChare
# See LICENSE.txt for details.

import sys, os
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..', '..')))


import asyncio
import blindx.misc as misc
from proofreader import Proofreader
from proofreader import check_chunk_match
import argparse
from tqdm import tqdm
import logging
import re
import io
import jaconv
import unicodedata
import difflib

BLUE = '\033[38;5;25m'         # é’ç³»å‰æ™¯ï¼ˆ.ansi21 ç›¸å½“ï¼‰
CYAN = '\033[36m'            # ã‚·ã‚¢ãƒ³ æ ¡æ­£ã®å…¥ã£ãŸã‚‚ã¨ãƒ†ã‚­ã‚¹ãƒˆ
YELLOW = '\033[38;5;226m'    # é»„è‰²ã€€æ ¡æ­£ç®‡æ‰€
SOFT_YELLOW = '\033[38;5;143m'
RED = '\033[31m'
BOLD = '\033[1m'
BOLD_RED = '\033[31;1m'  # å¤ªå­—ãƒ»èµ¤
RESET = '\033[0m'
RESUME = '\033[39m'  #è‰²ã‚’æ¨™æº–è‰²ã«

SCORE_THRESHOLD = 40  # â† å¥½ã¿ã«å¿œã˜ã¦èª¿æ•´å¯èƒ½ï¼ˆ60ã¨ã‹ã§ã‚‚å¯ï¼‰

def normalize(text):
    return unicodedata.normalize("NFKC", text.strip())

def split_output_text(output_text: str, expected_lines: int):
    if output_text.count('\\n') >= expected_lines - 1:
        # æ”¹è¡Œæ•°ãŒååˆ†ã‚ã‚‹å ´åˆã¯ \\n ã§åˆ†å‰²
        return output_text.split('\\n')
    else:
        # æ”¹è¡Œæ•°ãŒä¸è¶³ã—ã¦ã„ã‚‹å ´åˆã¯å¥ç‚¹ã§åˆ†å‰²
        lines = re.split(r'(?<=ã€‚)', output_text)
        return [line.strip() for line in lines if line.strip()]

def make_numbered_input(lines):
    """å…¥åŠ›è¡Œã« 1: æ–‡â€¦ ã¨ã„ã†å½¢å¼ã§ç•ªå·ã‚’æŒ¯ã£ã¦é€£çµã™ã‚‹"""
    return "\n".join(f"{i+1}: {line}" for i, line in enumerate(lines))

import re

def extract_numbered_output(output_text: str, expected_lines: int):
    """
    '1: ...\\n2: ...\\n3: ...' ã®ã‚ˆã†ãªå‡ºåŠ›ã‚’ã€è¡Œã”ã¨ã®ãƒªã‚¹ãƒˆã«å¤‰æ›
    """
    matches = re.findall(r'\d+\s*:\s*.*?(?=\n\d+\s*:|$)', output_text, flags=re.DOTALL)
    lines = [re.sub(r'^\d+\s*:\s*', '', m).strip() for m in matches]

    # è¡Œæ•°è£œæ­£
    if len(lines) < expected_lines:
        lines += [''] * (expected_lines - len(lines))
    elif len(lines) > expected_lines:
        lines = lines[:expected_lines]

    return lines


if __name__ == "__main__":

    logger = logging.getLogger(__name__)

    parser = argparse.ArgumentParser()
    parser.add_argument('-i', '--input', help='source filename', default='input.txt')
    parser.add_argument('-o', '--output', help='result filename', default='-')
    parser.add_argument('-e', '--echo', help='phrase')
    parser.add_argument('--hotwords', help='ã‚¹ãƒšãƒ¼ã‚¹åŒºåˆ‡ã‚Šã§HOTWORDSã‚’æŒ‡å®šï¼ˆçœç•¥å¯èƒ½ï¼‰', default='') 
    #    parser.add_argument('--encoding', choices=['utf-8', 'utf-16-le'], default='utf-8', help='text encoding') 
    #    parser.add_argument('--hotwords', nargs='+', help='hot words')
    parser.add_argument('--encoding', default='auto', help='text encoding (e.g., utf-8, utf-16-le, or auto)')
    parser.add_argument('--hotfile', help='hot file')
    parser.add_argument('--max_chars', default=0, type=int, help='max charsã€€ä½•æ–‡å­—è©°ã‚è¾¼ã‚€ã‹')
    parser.add_argument('--num_beams', default=3, type=int, help='max beams : ä½•å€‹å€™è£œã‚’å‡ºã™ã‹')
    args = parser.parse_args()

    def chunk_lines_by_char_limit(lines, max_chars=None):
        if max_chars is None or max_chars <= 0:
            # max_charsãŒ0ä»¥ä¸‹ãªã‚‰ã€1æ–‡ã”ã¨ã«ãƒãƒ£ãƒ³ã‚¯åŒ–
            return [([line], [idx]) for idx, line in enumerate(lines) if line.strip()] 

        chunks = []
        current_chunk = []
        current_length = 0
        current_indices = []

        for idx, line in enumerate(lines):
            line_length = len(line)

#            print(f'[DEBUG] line = {line[:30]}')
            
            # æœ€åˆã®è¡Œã¯ç„¡æ¡ä»¶ã«å…¥ã‚Œã‚‹
            if not current_chunk:
                current_chunk.append(line)
                current_indices.append(idx)
                current_length += line_length
                continue

            # max_chars ãŒ Noneï¼ˆåˆ¶é™ãªã—ï¼‰ãªã‚‰å¸¸ã«è¿½åŠ 
            if max_chars is None or current_length + line_length <= max_chars:
                current_chunk.append(line)
                current_indices.append(idx)
                current_length += line_length
            else:
                chunks.append((current_chunk.copy(), current_indices.copy()))
                current_chunk = [line]
                current_indices = [idx]
                current_length = line_length

        if current_chunk:
            chunks.append((current_chunk.copy(), current_indices.copy()))
        return chunks

    def chunk_lines_by_char_limit_old(lines, max_chars=None):
        if max_chars is None:
            max_chars = 256

        chunks = []
        current_chunk = []
        current_length = 0
        current_indices = []

        for idx, line in enumerate(lines):
            line_length = len(line)

            # ãƒãƒ£ãƒ³ã‚¯ãŒç©ºï¼ˆæœ€åˆã®è¡Œï¼‰ãªã‚‰ã€é•·ã•ç„¡è¦–ã§å…¥ã‚Œã‚‹
            if not current_chunk:
                current_chunk.append(line)
                current_indices.append(idx)
                current_length += line_length
                continue

            # ç¾åœ¨ã®ãƒãƒ£ãƒ³ã‚¯ã«åã¾ã‚‹ãªã‚‰è¿½åŠ 
            if current_length + line_length <= max_chars:
                current_chunk.append(line)
                current_indices.append(idx)
                current_length += line_length
            else:
                # å…¥ã‚ŒãŸã‚‰ã‚ªãƒ¼ãƒãƒ¼ã™ã‚‹ãŒã€ç¾åœ¨ã®ãƒãƒ£ãƒ³ã‚¯ãŒç©ºãªã‚‰å¼·åˆ¶çš„ã«å…¥ã‚Œã‚‹
                chunks.append((current_chunk.copy(), current_indices.copy()))
                current_chunk = [line]
                current_indices = [idx]
                current_length = line_length

        if current_chunk:
            chunks.append((current_chunk.copy(), current_indices.copy()))

        return chunks

    import chardet
    from pathlib import Path

    def read_lines(path_str, encoding='auto'):
        path = Path(path_str)
        raw = path.read_bytes()

        if encoding == 'auto':
            detected = chardet.detect(raw)
            encoding = detected.get('encoding') or 'utf-8'

        try:
            decoded = raw.decode(encoding)
        except UnicodeDecodeError:
            print(f"âš ï¸ decode error with {encoding}, fallback to utf-8 with ignore")
            decoded = raw.decode('utf-8', errors='ignore')

        return [line.lstrip('\ufeff') for line in decoded.splitlines()]

    def load_hotwords(filepath):
        path = Path(filepath)

        try:
            raw = path.read_bytes()
        except Exception as e:
            print(f"âŒ hotfile èª­ã¿è¾¼ã¿å¤±æ•—: {filepath} â†’ {e}")
            return []

        detected = chardet.detect(raw)
        encoding = detected.get("encoding") or "utf-8"

        try:
            text = raw.decode(encoding)
        except UnicodeDecodeError:
            print(f"âš ï¸ UnicodeDecodeErrorï¼ˆ{encoding}ï¼‰â†’ fallback to utf-8(errors='ignore')")
            text = raw.decode("utf-8", errors="ignore")

        hotwords = text.strip().split()
        return hotwords
    
    async def main():
        misc.set_logger('kousei', logging.WARN)
        RED = '\033[31m'
        BOLD = '\033[1m'
        RESET = '\033[0m'

        hotwords = [
            ' ', 'ã€€', 'ã€', ',', 'â€¦', '^â—†.*$',
            'ã€.*?ã€‘', 'ï¼».*?ï¼½', '^â—†.*$', '//.*$', '^;.*$',
        ]
        
        if args.hotwords:
            hotwords += args.hotwords
        if args.hotfile:
            hotwords += load_hotwords(args.hotfile)
        max_chars = int(args.max_chars) if args.max_chars else None

        if args.echo:
            lines = [args.echo]
        else:
            lines = read_lines(args.input, encoding=args.encoding) 
            #            lines = read_lines(args.input)
        if args.num_beams:
            num_of_beams = args.num_beams

        proofreader  = Proofreader()
        proofreaders = []

        await proofreader.start_async()
        await proofreader.set_pattern_async('|'.join(hotwords))
        await proofreader.set_replacement_async('$')

        if args.output == '-':
            progress_bar = None
            buffer = None
        else:
            buffer = io.StringIO()
            original_stdout = sys.stdout
            sys.stdout = buffer
            progress_bar = None

        fail_count = 0
        pass_count = 0

        chunks = chunk_lines_by_char_limit(lines, max_chars=max_chars)

        #        print(f"[DEBUG] ãƒãƒ£ãƒ³ã‚¯æ•° = {len(chunks)}")

        print(f'Max Chars : {max_chars}       Num Beams : {num_of_beams}')
        print(f"hotwords : {' '.join(hotwords)}")
        print('\n--------')

        # 1ãƒãƒ£ãƒ³ã‚¯ãšã¤å‡¦ç†
        for chunk_index, (chunk_lines, line_indices) in enumerate(chunks):
            input_text = "\n".join(chunk_lines)
            proofreader.input_text = input_text

            pass_flag = False
            output_scores = {}  # ã“ã‚Œã‚’æœ€åˆã«å®šç¾©ï¼

            import difflib

            input_lines = input_text.splitlines()
            num_lines = len(input_lines)
            output_texts_per_line = [[] for _ in range(num_lines)]

            # å…¨å‡ºåŠ›ã‚’åé›†
            output_texts_all = []
            for dict_index in range(proofreader.dict_count):
#                print(f'dict_index = {dict_index}')
                output_texts = await proofreader.test_async(input_text, dict_index , num_of_beams)
                output_texts_all.extend(output_texts)

            # å‡ºåŠ›ã”ã¨ã«å‡¦ç†
            for output_text, dict_name in output_texts_all:
                output_lines = output_text.split('\\n')

                # è¡Œæ•°ãŒå¤šã™ãã‚Œã°ã‚«ãƒƒãƒˆã€è¶³ã‚Šãªã‘ã‚Œã°ç©ºè¡Œè¿½åŠ 
                if len(output_lines) < num_lines:
                    output_lines += [''] * (num_lines - len(output_lines))
                elif len(output_lines) > num_lines:
                    output_lines = output_lines[:num_lines]

                cursor = 0  # æœ€åˆã®æ¢ç´¢ä½ç½®
                SIMILARITY_THRESHOLD = 0.2
                for out_line in output_lines:
                    out_line_clean = out_line.strip()
                    matched = False

                    for i in range(cursor, num_lines):
                        in_line_clean = input_lines[i].strip()
                        sim = difflib.SequenceMatcher(None, in_line_clean, out_line_clean).ratio()

                        if sim >= SIMILARITY_THRESHOLD:
#                            print(f"[MATCH] è¡Œ {i}: sim={sim:.2f} | å…¥åŠ›: {in_line_clean} | å‡ºåŠ›: {out_line_clean}")
                            output_texts_per_line[i].append((out_line, dict_name))
                            cursor = i + 1  # æ¬¡ã®æ¢ç´¢ã¯ãã®æ¬¡ã‹ã‚‰
                            matched = True
                            break

#                    if not matched:
#                        print(f"[SKIP] é¡ä¼¼åº¦ä¸è¶³ã§ç ´æ£„: '{out_line}'")
                        
            # ã™ã¹ã¦ã®å‡ºåŠ›è¡Œã®ãƒãƒƒãƒå‡¦ç†ãŒçµ‚ã‚ã£ãŸã‚ã¨
            # â†“ ã“ã®è¡Œã®å¾Œã«å…¥ã‚Œã‚‹
            # for output_text, dict_name in output_texts_all:
            #     ...

            proofreaders = []   # åˆæœŸåŒ–
            # â˜…ã“ã“ã§ Proofreader ã‚’è¡Œã”ã¨ã«ã¾ã¨ã‚ã¦æ§‹ç¯‰
            for i in range(num_lines):
                pr = Proofreader()
                pr.input_text = input_lines[i]
                pr.zenkaku_input_text = normalize(input_lines[i])
                pr.output_texts = output_texts_per_line[i]  # â† ã™ã¹ã¦ã®è¾æ›¸å‡ºåŠ›ãŒã“ã“ã«ã¾ã¨ã‚ã¦å…¥ã‚‹ï¼
#                print(f'output texts = {output_texts_per_line[i]}')
                proofreaders.append(pr)
                #                print(f"[DEBUG] proofreaders ã«è¿½åŠ : è¡Œ {i}: {pr.input_text}")


            BLUE = '\033[38;5;25m'         # é’ç³»å‰æ™¯ï¼ˆ.ansi21 ç›¸å½“ï¼‰
            CYAN = '\033[36m'            # ã‚·ã‚¢ãƒ³ æ ¡æ­£ã®å…¥ã£ãŸã‚‚ã¨ãƒ†ã‚­ã‚¹ãƒˆ
            YELLOW = '\033[38;5;226m'    # é»„è‰²ã€€æ ¡æ­£ç®‡æ‰€
            BOLD_RED = '\033[31;1m'  # å¤ªå­—ãƒ»èµ¤
            RESET = '\033[0m'
            BOLD = '\033[1m'
            RESUME = '\033[39m'  #è‰²ã‚’æ¨™æº–è‰²ã«
                

            #ã“ã“ã‹ã‚‰åˆ¤å®š
            for i, pr in enumerate(proofreaders):
#                print(f"\nline : {i} {pr.input_text}")
                output_scores = {}  # åˆæœŸåŒ–ã¯ã“ã“ã§ä¸€åº¦ã ã‘
                pass_flag = False
                seen_texts = set() 
                for output_text, dictionary_name in pr.output_texts:
                    try:
                        zenkaku_output_text = pr.concat_output_text(output_text)
                        score, chunk_match = pr.get_score(zenkaku_output_text)
                        if(score == 100):
                            pass_flag = True
                        match = re.match(r'T(\d+)', dictionary_name)
                        if match:
                            model_index = int(match.group(1))
                            model_name = pr.t5_names[model_index]
                        else:
                            model_name = dictionary_name

                        if zenkaku_output_text not in seen_texts:
                            output_scores[(zenkaku_output_text, model_name)] = score
                            seen_texts.add(zenkaku_output_text)
                            
                    except Exception as e:
                        print(f"[ERROR] å¤‰æ›å¤±æ•—: {e} / output_text={output_text} / dict={dictionary_name}")
                chunk_match,unmatched_chunks = check_chunk_match(pr)
                seen_texts = set()                         
                # ã‚¹ã‚³ã‚¢ã§ä¸¦ã³æ›¿ãˆï¼ˆé«˜å¾—ç‚¹é †)
                sorted_scores = sorted(output_scores.items(), key=lambda x: x[1], reverse=True)
                #                print(f'[DEBUG] sorted score = {sorted_scores}')
                #                has_high_score = any(s >= 80 for s in output_scores.values())
                has_high_score = True
#                print(f' [DEBUG]  has_high_score {has_high_score}  chunk_match {chunk_match}')
                if(pass_flag or chunk_match):
#                    print(f'PASS: pass_flag {pass_flag}  has_high_score {has_high_score}  chunk_match {chunk_match}')
                    print(f'{pr.input_text}')
                    pass_count += 1
                else:
#                    print(f'{pr.highlight_diff(pr.input_text, zenkaku_output_text)}')
                    print(f'{BOLD}{CYAN}{pr.highlight_unmatched_chunks(pr.input_text, unmatched_chunks)}{RESET}')
#                    print(f' FAIL: has_high_score {has_high_score}  chunk_match {chunk_match}')
                    fail_count += 1
                    for (zenkaku_output_text, model_name), score in sorted_scores:
                        if zenkaku_output_text in seen_texts:
                            continue  # ã™ã§ã«å‡ºåŠ›æ¸ˆã¿ã®æ–‡ã¯ã‚¹ã‚­ãƒƒãƒ—
                        seen_texts.add(zenkaku_output_text)
                        #                        print(f'      {score:3.0f}: {zenkaku_output_text} [{model_name}]')
                        if score < 50:
                            continue
                        if has_high_score == True:
                            print(f'\033[31m{pr.highlight_diff(zenkaku_output_text,input_text)}\033[0m : {score:3.0f} [{model_name}] ')
                        else:
                            print(f'{RED}{zenkaku_output_text}{RESET} : {RED}{BOLD}{score:3.0f}{RESET} [{model_name}] ')

                            
        if progress_bar:
            progress_bar.set_postfix({'æ€ªã—ã„è¡Œ': fail_count})

        print(f'\n       FAIL COUNT :  {RED}{fail_count:3.0f}{RESUME}    PASS COUNT : {CYAN}{pass_count:3.0f}{RESUME}        REDUCTION : {SOFT_YELLOW}{(pass_count)/(pass_count + fail_count)*100:.1f}{RESUME} %       \n')

        if buffer:        
            sys.stdout = sys.__stdout__  # å…ƒã«æˆ»ã™
            from ansi2html import Ansi2HTMLConverter

            conv = Ansi2HTMLConverter()
            html_output = conv.convert(buffer.getvalue(), full=True)

            # ğŸ’¡ <meta charset="UTF-8"> ã‚’ head ã«è¿½åŠ ï¼ˆã‚ã‚Œã°ï¼‰
            if "<head>" in html_output:
                html_output = html_output.replace("<head>", "<head><meta charset='UTF-8'>", 1)

            with open(args.output, 'w', encoding='utf-8') as f:
                f.write(html_output) 
                            
        await proofreader.shutdown_async()


        
# ä»–ã®é–¢æ•°ã‚„ã‚¯ãƒ©ã‚¹ã®å¤–ã«æ›¸ãï¼
# ãƒ•ã‚¡ã‚¤ãƒ«ã®ä¸€ç•ªæœ€å¾Œã§OKï¼

if __name__ == "__main__":
    import asyncio
    asyncio.run(main())        

#        asyncio.run(main())

