# MIT License (c) 2024, 2025 Masakazu Suzuoki, AxTecChare
# See LICENSE.txt for details.

import sys, os
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..', '..')))

import asyncio
from blindx.kanhira import Kanhira
from blindx.remote_inference import RemoteInference
from rapidfuzz import fuzz
import jaconv
import MeCab
import difflib


import unicodedata
import re
from difflib import SequenceMatcher

BLUE = '\033[38;5;25m'         # é’ç³»å‰æ™¯ï¼ˆ.ansi21 ç›¸å½“ï¼‰
CYAN = '\033[36m'            # ã‚·ã‚¢ãƒ³ æ ¡æ­£ã®å…¥ã£ãŸã‚‚ã¨ãƒ†ã‚­ã‚¹ãƒˆ
YELLOW = '\033[38;5;226m'    # é»„è‰²ã€€æ ¡æ­£ç®‡æ‰€
SOFT_YELLOW = '\033[38;5;143m'
RED = '\033[31m'
BOLD = '\033[1m'
BOLD_RED = '\033[31;1m'  # å¤ªå­—ãƒ»èµ¤
RESET = '\033[0m'
RESUME = '\033[39m'  #è‰²ã‚’æ¨™æº–è‰²ã«

def normalize_ellipsis(text: str) -> str:
    # åŠè§’ãƒ”ãƒªã‚ªãƒ‰3ã¤ä»¥ä¸Š â†’ å…¨è§’ä¸‰ç‚¹ãƒªãƒ¼ãƒ€ï¼ˆU+2026ï¼‰ã«ç½®æ›ï¼ˆ2ã¤é€£ç¶šã«çµ±ä¸€ï¼‰
    import re
    return re.sub(r'\.{3,}', 'â€¦â€¦', text)

def normalize_and_compare(a, b):
#    print(f' normalize and compare :  IN1:{a}  IN2:{b}') 
    a_norm = unicodedata.normalize('NFKC', a)
    b_norm = unicodedata.normalize('NFKC', b)
    return a_norm == b_norm

def normalize_punctuation(text):
    # ã€Œâ€¦ã€ã‚’ã€Œ.ã€ã«å¤‰æ›ã€ã€Œã€‚ã€ã¯ãã®ã¾ã¾
    text = text.replace('â€¦', '.')
    # ã€Œ.ã€ãŒé€£ç¶šã—ã¦ã„ã‚‹ç®‡æ‰€ã‚’ã€Œ...ã€ã«æ­£è¦åŒ–ï¼ˆé•·ã•çµ±ä¸€ï¼‰
    text = re.sub(r'\.{2,}', '...', text)
    return text


def is_similar(text1, text2, threshold=0.9):
    norm1 = normalize_punctuation(text1)
    norm2 = normalize_punctuation(text2)
    # é¡ä¼¼åº¦è¨ˆç®—
    similarity = SequenceMatcher(None, norm1, norm2).ratio()
    return similarity >= threshold, similarity

def normalize_text(text):
    text = remove_scores(text)
    text = jaconv.z2h(text, kana=False, digit=True, ascii=True)    
    text = jaconv.kata2hira(text)  # ä»»æ„
    return re.sub(r'[ï¼ï¼Ÿ!?\ã€‚ã€ï¼ã€Œã€ï¼ˆï¼‰ã€ã€ï¼»ï¼½ã€ã€‘â€¦â€¥ãƒ»ãƒ¼"\'\s]', '', text)

import MeCab
import platform
import os

def get_mecab_dic_path():
    if platform.system() == 'Windows':
        return "C:/msys64/mingw64/lib/mecab/dic/mecab-ipadic-neologd"
    elif platform.system() == 'Darwin':  # macOS
        return '/opt/homebrew/lib/mecab/dic/mecab-ipadic-neologd'
    else:
        raise RuntimeError("Unsupported OS")


# ğŸ”½ ã‚°ãƒ­ãƒ¼ãƒãƒ«å¤‰æ•°ã¨ã—ã¦å®šç¾©
DIC_PATH = get_mecab_dic_path()
# ï¼ˆä»»æ„ï¼‰å­˜åœ¨ãƒã‚§ãƒƒã‚¯ã‚‚å…¥ã‚Œã‚‹ã¨å®‰å¿ƒ
if not os.path.exists(os.path.join(DIC_PATH, "dicrc")):
    raise FileNotFoundError(f"MeCabè¾æ›¸ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {DIC_PATH}")

import re

def remove_scores(text):
    # ã‚¹ã‚³ã‚¢ï¼ˆ,1.00ãªã©ï¼‰ã‚’å‰Šé™¤
    text = re.sub(r',\d+(?:\.\d+)?', '', text)
    # ã‚«ãƒ³ãƒè‡ªä½“ã‚‚é™¤å»ï¼ˆèªã¨èªã®é–“ã‚’ã¤ãªã’ã‚‹ï¼‰
    return text.replace(',', '')

def parse_blindx_texts(blindx_texts):
    """
    blindx_texts å½¢å¼:
    ä¾‹: T,1.00,he,1.00,B,1.00,e,1.00,... : ãƒˆãƒ¼ã‚¯ãƒ³ã¨ã‚¹ã‚³ã‚¢ãŒã‚«ãƒ³ãƒã§äº¤äº’ã«ä¸¦ã‚“ã§ã„ã‚‹
    ã‚³ãƒ­ãƒ³ : ã§ãƒ“ãƒ¼ãƒ ã”ã¨ã«åŒºåˆ‡ã‚‰ã‚Œã¦ã„ã‚‹

    æˆ»ã‚Šå€¤: ["TheBeginning", "TheBegin", ...] ã®ã‚ˆã†ãªãƒ†ã‚­ã‚¹ãƒˆãƒªã‚¹ãƒˆ
    """
    outputs = blindx_texts.split(':')
    texts = []
    for output in outputs:
        items = output.split(',')
        tokens = items[::2]  # å¶æ•°ç•ªç›®ãŒãƒˆãƒ¼ã‚¯ãƒ³
        text = ''.join(tokens)
        texts.append(text)
    return texts


def check_chunk_match(pr):
#    dic_path = get_mecab_dic_path()
#    if not os.path.exists(os.path.join(dic_path, "dicrc")):
#        raise FileNotFoundError(f"MeCabè¾æ›¸ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {dic_path}")

    try:
        
#        wakati_tagger = MeCab.Tagger("-Owakati")
        wakati_tagger = MeCab.Tagger(f"-Owakati -d {DIC_PATH}") 
#        wakati_tagger = MeCab.Tagger("-Owakati -d /opt/homebrew/lib/mecab/dic/mecab-ipadic-neologd")
        norm_input = normalize_text(pr.input_text)
        input_chunks = wakati_tagger.parse(norm_input)

        if input_chunks is None:
            raise ValueError("MeCab parse returned None for input")

        input_chunks = input_chunks.strip().split()
#        print(f"[DEBUG] input chunks: {input_chunks}")

        # å…¨ã¦ã® output_text ã‚’1ã¤ã«ã¾ã¨ã‚ã¦åˆ†ã‹ã¡æ›¸ãã—ã€çµåˆ
        all_output_chunks = []
        for output_text, _ in pr.output_texts:
            norm_output = normalize_text(output_text)
            out_chunks = wakati_tagger.parse(norm_output)
            if not out_chunks:
                continue
            out_chunks = out_chunks.strip().split()
            all_output_chunks.extend(out_chunks)

        joined_output = ''.join(all_output_chunks)
#        print(f"[DEBUG] joined all output = {joined_output}")

        unmatched_chunks = [chunk for chunk in input_chunks if chunk not in joined_output]

        if not unmatched_chunks:
#            print(f"[DEBUG] âœ… chunk_match æˆç«‹")
            return True, []
        else:
#            print(f"[DEBUG] âŒ unmatched chunks: {unmatched_chunks}")
            return False, unmatched_chunks  # ãƒãƒƒãƒå¤±æ•—ã€unmatched ã®ãƒªã‚¹ãƒˆä»˜ã

    except Exception as e:
        print(f"[ERROR] MeCab chunk_match failed: {e}")
        return False


class Proofreader():

    def __init__(self):
        self.kanhira = Kanhira()
        self.inference = RemoteInference()
        self.output_texts = []
        
        dic_path = get_mecab_dic_path()
        if not os.path.exists(os.path.join(dic_path, "dicrc")):
            raise FileNotFoundError(f"MeCabè¾æ›¸ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {dic_path}")

#        self.wakati_tagger = MeCab.Tagger("-Owakati")
        wakati_tagger = MeCab.Tagger(f"-Owakati -d {DIC_PATH}") 
#        wakati_tagger = MeCab.Tagger("-Owakati -d /opt/homebrew/lib/mecab/dic/mecab-ipadic-neologd")
#        print(f'[DEBUG] Neologd Start ')
        self.passed_index = 0

    @property
    def t5_names(self):
        return self.inference.t5_names

    
    async def start_async(self):
        await self.inference.start_async()
#        self.dict_names = await self.inference.send_recv_async('query:', 'dict_names')

        # dict_namesã‚’å–å¾—ã—ã¦ split(':') ã§ãƒªã‚¹ãƒˆã«å¤‰æ›
        raw_dict_names = await self.inference.send_recv_async('query:', 'dict_names')
        self.dict_names = raw_dict_names.split(':')
        self.dict_count = len(self.dict_names)  # âœ… â†ã“ã‚Œã§OK        
#        print("[DEBUG] dict_names =", self.dict_names)
        self.dict_count = len(self.dict_names)
#        self.dict_count = len(self.dict_names.split(':'))

    async def shutdown_async(self):
        await self.inference.shutdown_async()
        
    def concat_output_text(self, output_text):
        items = output_text.split(',')
        tokens = items[::2]
        probs = items[1::2]
        result_text = ''.join(tokens)
        return jaconv.h2z(result_text, ascii=True, digit=True)


    # æ–‡ç¯€ä¸€è‡´åˆ¤å®šï¼šoutput_texts ã®ä¸­ã« input_text ã®æ–‡ç¯€ã‚’ã™ã¹ã¦å«ã‚€ã‚‚ã®ãŒä¸€ã¤ã§ã‚‚ã‚ã‚‹ã‹ï¼Ÿ
    def get_score(self, zenkaku_output_text):
        chunk_match = False
        try:
            # å…¥åŠ›ã®å­˜åœ¨ãƒã‚§ãƒƒã‚¯ã¨å‰å‡¦ç†
            if self.zenkaku_input_text is None:
                raise ValueError("zenkaku_input_text ãŒ None ã§ã™")
            if zenkaku_output_text is None:
                raise ValueError("zenkaku_output_text ãŒ None ã§ã™")

            input_text = self.zenkaku_input_text.strip()
            output_text = zenkaku_output_text.strip()

            norm_input = normalize_ellipsis(input_text)
            norm_output = normalize_ellipsis(output_text)

#            print(f"[DEBUG:get_score] input = {repr(input_text)}")
#            print(f"[DEBUG:get_score] output = {repr(output_text)}")

            # å®Œå…¨ä¸€è‡´ã®å ´åˆã¯å³åº§ã«100ç‚¹
            if norm_input == norm_output:
#                print("[DEBUG:get_score] å®Œå…¨ä¸€è‡´ â†’ score=100")
                return 100, True



            # é¡ä¼¼åº¦ã‚¹ã‚³ã‚¢ã®è¨ˆç®—
            try:
                overall_score = fuzz.ratio(norm_input, norm_output)
#                print(f"[DEBUG:get_score] fuzzy score = {overall_score}")
            except Exception as e:
#                print(f"[ERROR] fuzz.ratio failed: {e}")
                raise

            # æ–‡ç¯€ä¸€è‡´ãƒã‚§ãƒƒã‚¯ï¼ˆMeCabï¼‰
            try:
                #wakati_tagger = MeCab.Tagger("-Owakati")
                wakati_tagger = MeCab.Tagger(f"-Owakati -d {DIC_PATH}") 
#                wakati_tagger = MeCab.Tagger("-Owakati -d /opt/homebrew/lib/mecab/dic/mecab-ipadic-neologd")
                orig_chunks = wakati_tagger.parse(norm_input)
                out_chunks = wakati_tagger.parse(norm_output)

                if orig_chunks is None or out_chunks is None:
                    raise ValueError("MeCab parse returned None")

                orig_chunks = orig_chunks.strip().split()
                out_chunks = out_chunks.strip().split()
                chunk_match = len(set(orig_chunks) & set(out_chunks)) > 0
            except Exception as e:
                print(f"[ERROR] MeCab parse failed: {e}")
                chunk_match = False

 #           print(f"[DEBUG:get_score] returning score = {overall_score}, chunk_match = {chunk_match}")
            return overall_score, chunk_match

        except Exception as e:
            print(f"[ERROR] get_score å†…éƒ¨ã‚¨ãƒ©ãƒ¼: {e}")
            raise

    
    def all_input_chunks_matched_old(self, input_text, output_texts):
        input_chunks = self.wakati_tagger.parse(input_text).strip().split()
        input_chunks = [normalize_text(chunk) for chunk in input_chunks if chunk.strip()]

        # å‡ºåŠ›å´ã® merged text ç¾¤ã‚’ã™ã¹ã¦æ­£è¦åŒ–ã—ã¦ç”¨æ„
        merged_outputs = [
            normalize_text(remove_scores(output_text))
            for output_text, _ in output_texts
        ]

        # å„ chunk ãŒå‡ºåŠ›ç¾¤ã®ã©ã‚Œã‹1ã¤ã«å«ã¾ã‚Œã¦ã„ã‚‹ã‹ç¢ºèª
        unmatched_chunks = []
        for chunk in input_chunks:
            if not any(chunk in merged for merged in merged_outputs):
#                print(f"[DEBUG] unmatched chunk: '{chunk}'")
                unmatched_chunks.append(chunk)

        if unmatched_chunks:
#            print(f"[DEBUG] unmatched chunks: {unmatched_chunks}")
            return False

#        print(f"[DEBUG] all chunks matched in some output")
        return True


    import difflib

    def highlight_unmatched_chunks(self, input_text, unmatched_chunks):
        highlighted = input_text
        for chunk in unmatched_chunks:
            # chunk ãŒè¤‡æ•°å›å‡ºã‚‹å¯èƒ½æ€§ãŒã‚ã‚‹ãŸã‚ã€ã™ã¹ã¦ç½®æ›ï¼ˆæœ€åˆã®ã ã‘ãªã‚‰ 1 ã‚’æŒ‡å®šï¼‰
            highlighted = highlighted.replace(chunk, f"{SOFT_YELLOW}{chunk}{CYAN}")
        return highlighted
    
    def highlight_diff(self,a, b):
        matcher = difflib.SequenceMatcher(None, a, b)
        result = []
        
        for tag, i1, i2, j1, j2 in matcher.get_opcodes():
            if tag == 'equal':
                result.append(a[i1:i2])
            elif tag == 'replace' or tag == 'delete':
                result.append(f'{SOFT_YELLOW}{a[i1:i2]}{RED}')
                # 'insert' ã¯ A ã«ã¯ãªã„éƒ¨åˆ†ãªã®ã§ç„¡è¦–
        return ''.join(result)
            
    async def test_async(self, input_text, dict_index, num_beams=2):
        self.input_text = input_text
        self.output_texts = []

        if(input_text == ''):   #ä½•ã‚‚ãªã„ã‚‚ã®ã‚’æ”¾ã‚Šè¾¼ã‚€ã¨ã‚¨ãƒ©ãƒ¼ã«ãªã‚‹ãŸã‚
            return []

        try:
            # å…¥åŠ›ã®å‰å‡¦ç†ï¼ˆã‚³ãƒ­ãƒ³ã®ã‚¨ã‚¹ã‚±ãƒ¼ãƒ—ãªã©ï¼‰
            escaped_input_text = input_text.replace(':', '<COLON>')
            self.hiragana_text = self.kanhira.convert(escaped_input_text)
            self.zenkaku_input_text = jaconv.h2z(input_text, ascii=True, digit=True)

            # ãƒ¢ãƒ‡ãƒ«å‘¼ã³å‡ºã—
            dict_cmd = f'T{dict_index}:{num_beams}+:'
            blindx_texts = await self.inference.send_recv_async(dict_cmd, self.hiragana_text)

            # ãƒ‘ãƒ¼ã‚¹
            parsed_outputs = parse_blindx_texts(blindx_texts)
            if not parsed_outputs:
                print(f"[WARN] å‡ºåŠ›ãªã—: dict={self.dict_names[dict_index]}")
                return []

            # å‡ºåŠ›æ•´å½¢
            for output_text in parsed_outputs:
                if not output_text or output_text.strip().startswith("<pad>"):
#                    print(f"[SKIP] ç„¡åŠ¹å‡ºåŠ›ï¼ˆpadã®ã¿ï¼Ÿï¼‰: dict={self.dict_names[dict_index]}, chunk={input_text[:30]}")
                    continue
                output_text = output_text.replace('<COLON>', ':')
                self.output_texts.append((output_text, self.dict_names[dict_index]))
                
            return self.output_texts

        except Exception as e:
            print(f"[ERROR] test_async failed: {e}")
            return []

    async def set_pattern_async(self, pattern):
        self.kanhira.set_pattern(pattern)
        await self.inference.send_recv_async('set:wrapper_pattern:', pattern)

    async def set_replacement_async(self, replacement):
        self.kanhira.set_replacement(replacement)
        await self.inference.send_recv_async('set:wrapper_replacement:', replacement)


from ansi2html import Ansi2HTMLConverter

def save_colored_output_as_html(ansi_text: str, output_html_path: str):
    conv = Ansi2HTMLConverter()
    html = conv.convert(ansi_text, full=True)
    with open(output_html_path, "w", encoding="utf-8") as f:
        f.write(html)

